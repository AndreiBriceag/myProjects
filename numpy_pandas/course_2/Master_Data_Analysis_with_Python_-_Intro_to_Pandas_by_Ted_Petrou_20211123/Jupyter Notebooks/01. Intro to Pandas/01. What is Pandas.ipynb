{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is pandas?\n",
    "\n",
    "pandas is one of the most popular open source data exploration libraries currently available. It gives its users the power to explore, query, transform, aggregate, and visualize **tabular** data. Tabular refers to data that is two-dimensional, consisting of rows and columns. Commonly, we refer to this organized structure of data as a **table**. pandas is the tool that we will use to analyze data in nearly every chapter of this book.\n",
    "\n",
    "![1]\n",
    "\n",
    "## Why pandas and not xyz?\n",
    "\n",
    "In this current age of data explosion, there are now dozens of other tools that have many of the same capabilities as the pandas library. However, there are many aspects of pandas that make it an attractive choice for data analysis and it continues to have one of the fastest growing user bases.\n",
    "\n",
    "* It's a Python library and integrates well with the other popular data science libraries such as numpy, scikit-learn, statsmodels, matplotlib, and seaborn.\n",
    "* It is nearly self-contained in that lots of functionality is built into one package. This contrasts with R, where many packages are needed to obtain the same functionality.\n",
    "* The community is excellent. Looking at Stack Overflow, for example, there are [many ten's of thousands of][2] pandas questions. If you need help, you are nearly guaranteed to find it quickly. \n",
    "\n",
    "### Why is it named after an East Asian bear?\n",
    "\n",
    "The pandas library was begun by Wes McKinney in 2008 while working at the hedge fund AQR. In the financial world, it is common to refer to tabular data as 'panel data' which smashed together becomes pandas. If you are really interested in the history, hear it from the creator [himself][3].\n",
    "\n",
    "### Python already has data structures to handle data, why do we need another one?\n",
    "\n",
    "Even though Python is a high-level language, its primary built-in data structure to contain a sequence of values, lists, are not built for scientific computing. Lists are a general purpose data structure that can store any object of any type and are not optimized for tabular data analysis. Python lacks a built-in data structure that contains homogeneous data types for fast numerical computation. This data structure, usually referred to as an 'array' in most languages, is provided by the numpy third-party library.\n",
    "\n",
    "### pandas is built directly on numpy\n",
    "\n",
    "[numpy][4] ('numerical Python') is the most popular third-party Python library for scientific computing and forms the foundation for dozens of others, including pandas. numpy's primary data structure is an n-dimensional array which is much more powerful than a Python list and with much better performance for numerical operations.\n",
    "\n",
    "All of the data in pandas is stored in numpy arrays. That said, it isn't necessary to know much about numpy when learning pandas. You can think of pandas as a higher-level, easier to use interface for doing data analysis than numpy. It is a good idea to eventually learn numpy, but for most data analysis tasks, pandas will be the right tool.\n",
    "\n",
    "## pandas operates on tabular data\n",
    "\n",
    "There are numerous formats for data, such as XML, JSON, CSV, Parquet, raw bytes, and many others. pandas has the capability to read in many different formats of data and always converts it to tabular form. pandas is built just for analyzing this rectangular, deceptively normal concept of data. pandas is not a suitable library for handling data in more than two-dimensions. It's focus is strictly on data that is one or two dimensions.\n",
    "\n",
    "### The DataFrame and Series\n",
    "\n",
    "The DataFrame and Series are the two primary pandas objects that we use throughout this book.\n",
    "\n",
    "* **DataFrame** - A two-dimensional data structure that looks like any other rectangular table of data with rows and columns.\n",
    "* **Series** - A single dimension of data. It is analogous to a single column of data or a one-dimensional array.\n",
    "\n",
    "## pandas examples\n",
    "\n",
    "The rest of this chapter contains examples of common data analysis tasks with pandas. There are one or two examples from each of the following major areas of the library:\n",
    "\n",
    "* Reading data\n",
    "* Filtering data\n",
    "* Aggregating methods\n",
    "* Non-Aggregating methods\n",
    "* Aggregating within groups\n",
    "* Cleaning data\n",
    "* Joining data\n",
    "* Time series analysis\n",
    "* Visualization\n",
    "\n",
    "The goal is to give you a broad overview of what pandas is capable of doing. You are not expected to understand the syntax, but rather, get a few ideas of what you can expect to accomplish when using pandas. Explanations will be brief, but hopefully provide just enough information so that you can understand the end result.\n",
    "\n",
    "### The `head` method\n",
    "\n",
    "Many of the last lines of code end with the `head` method. By default, this method returns the first five rows of the DataFrame or Series that calls it. The purpose of this method is to limit the output so that it easily fits on a screen or page in a book. If the `head` method is not used, then pandas displays the first and last 5 rows of data by default (or all the rows if the DataFrame has 60 or less). To reduce output even further (to save space on the screen), an integer (usually 3) will be passed to the `head` method. This integer controls the number of rows returned.\n",
    "\n",
    "## Which pandas version to use?\n",
    "\n",
    "The pandas library is under constant development and regularly releases new versions. Currently, pandas is on major version **1**, which was released in January, 2020. Before major version 1, pandas was on version **0**. Python libraries use the form **a.b.c** for version numbering where **a** represents the **major** version number. It is increased whenever there are major changes, with some being backward incompatible. **b** represents the **minor** version number and is incremented for smaller backward-compatible changes and enhancements. **c** represents the **micro** version number and is incremented mainly for bug fixes.\n",
    "\n",
    "Often, only the major and minor version are written when speaking about the version of pandas as the micro version isn't all that important. pandas has a history of releasing around two or three minor versions per year. In order to run all of the code in this book, you need to be running **pandas 1.0** or greater.\n",
    "\n",
    "### Import pandas and verify version number\n",
    "\n",
    "Let's import pandas and verify it's version by accessing the special attribute `__version__`. If you are running any version of pandas less than 1.0 (such as 0.25 or below), then you'll need to exit the Jupyter Notebook, return to the command line and run `conda update pandas`.\n",
    "\n",
    "[1]: images/pandas_logo.png\n",
    "[2]: http://stackoverflow.com/questions/tagged/pandas\n",
    "[3]: https://www.youtube.com/watch?v=kHdkFyGCxiY\n",
    "[4]: http://www.numpy.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "\n",
    "Multiple datasets are used during the rest of this chapter. The `read_csv` function is able to read in data stored in plain text that is separated by a delimiter. By default, the delimiter is a comma. Below, we read in public bike usage data from the city of Chicago into a pandas DataFrame named `bikes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = pd.read_csv('../data/bikes.csv')\n",
    "bikes.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering data\n",
    "\n",
    "pandas can filter the rows of a DataFrame based on whether the values in that row meet a condition. For instance, we can select only the rides that had a `tripduration` greater than 5,000 (seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Condition\n",
    "\n",
    "This example is a single condition that gets tested for each row. Only the rows that meet this condition are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = bikes['tripduration'] > 5000\n",
    "bikes[filt].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Conditions\n",
    "\n",
    "We can test for multiple conditions in a single row. The following example returns rides by females **and** have a `tripduration` greater than 5,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt1 = bikes['tripduration'] > 5000\n",
    "filt2 = bikes['gender'] == 'Female'\n",
    "filt = filt1 & filt2\n",
    "bikes[filt].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example has multiple conditions but only requires that one of the conditions is true. It returns all the rows where either the rider is female **or** the `tripduration` is greater than 5,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = filt1 | filt2\n",
    "bikes[filt].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `query` method\n",
    "\n",
    "The `query` method provides an alternative and often more readable way to filter data than the above. All three filtering examples from above may be duplicated with `query`. A string representing the condition is passed to the `query` method to filter the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.query('tripduration > 5000').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.query('tripduration > 5000 and gender==\"Female\"').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.query('tripduration > 5000 or gender==\"Female\"').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating methods\n",
    "\n",
    "The technical definition of an **aggregation** is when a sequence of values is summarized by a **single** number. For example, `sum`, `mean`, `median`, `max`, and `min` are all examples of aggregation methods. By default, calling these methods on a pandas DataFrame applies the aggregation to each column. Below, we use a dataset containing San Francisco employee compensation information. Only a subset of the columns are initially read into the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['salaries', 'overtime', 'other salaries', 'retirement', 'health and dental']\n",
    "sf_emp = pd.read_csv('../data/sf_employee_compensation.csv', usecols=cols)\n",
    "sf_emp.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the `mean` method returns the mean of each column. The result is then rounded to the nearest thousand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_emp.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas allows you to aggregate rows as well. The `axis` parameter may be used to change the direction of the aggregation. This returns the total compensation for each employee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_emp.sum(axis=1).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-aggregating methods\n",
    "\n",
    "There are methods that perform some calculation on the DataFrame that do not aggregate the data and usually preserve the shape of the DataFrame. For example, the `round` method rounds each number to a given decimal place. Here, we round each value in the DataFrame to the nearest thousand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_emp.round(-3).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating within groups\n",
    "\n",
    "Above, we performed aggregations on the entire DataFrame. We can instead perform aggregations within groups of the data. Below we use an insurance dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ins = pd.read_csv('../data/insurance.csv')\n",
    "ins.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest aggregations is the frequency of occurrence of all the unique values within a single column. This is performed below with the `value_counts` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of unique values in a single column\n",
    "\n",
    "Here, we count the occurrence of each individual `region`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins['region'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single aggregation function\n",
    "\n",
    "Let's say we wish to find the mean charges for each of the unique values in the `sex` column. The `groupby` method creates groups based on the given grouping column before applying the aggregation. In this example, we return the mean charges for each sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.groupby('sex').agg(mean_charges=('charges', 'mean')).round(-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple aggregation functions\n",
    "\n",
    "pandas allows us to perform multiple aggregations at the same time. Below, we calculate the mean and max of the `charges` column as well as count the number of non-missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.groupby('sex').agg(mean_charges=('charges', 'mean'),\n",
    "                       max_charges=('charges', 'max'),\n",
    "                       count_charges=('charges', 'count')).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple grouping columns\n",
    "\n",
    "pandas allows us to form groups based on multiple columns. In the below example, each unique combination of `sex` and `region` form a group. For each of these groups, the same aggregations as above are performed on the `charges` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.groupby(['sex', 'region']).agg(mean_charges=('charges', 'mean'),\n",
    "                                   max_charges=('charges', 'max'),\n",
    "                                   count_charges=('charges', 'count')).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Tables\n",
    "\n",
    "We can reproduce the exact same output as above in a different shape with the `pivot_table` method. It groups and aggregates the same way as `groupby`, but places the unique values of one of the grouping columns as the new columns in the resulting DataFrame. Notice that pivot tables make for easier comparisons across groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = ins.pivot_table(index='sex', columns='region', \n",
    "                     values='charges', aggfunc='mean').round(0)\n",
    "pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Styling DataFrames\n",
    "\n",
    "pandas enables you to style DataFrames in various ways to provide emphasis on particular cells. Below, the maximum value of each column is highlighted, a comma is added to separate the digits, and decimals are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.style.highlight_max().format(r'{:,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data\n",
    "\n",
    "Many datasets need to be cleaned before analyzed. pandas provides many tools to prepare data for further analysis.\n",
    "\n",
    "### Options in the `read_csv` function\n",
    "\n",
    "Below, we read in a new dataset on plane crashes. Notice all the question marks. They represent missing values, but pandas will read them in as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pd.read_csv('../data/tidy/planecrashinfo.csv')\n",
    "pc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_csv` function has dozens of options to help read in messy data. One of the options allows you to convert a particular string to missing values. Notice that all of the question marks are now labeled as `NaN` (not a number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pd.read_csv('../data/tidy/planecrashinfo.csv', na_values='?')\n",
    "pc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String manipulation\n",
    "\n",
    "Often times there is data trapped within a string column that you will need to extract. The `aboard` column appears to have three distinct pieces of information; the total number of people on board, the number of passengers, and the number of crew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aboard = pc['aboard']\n",
    "aboard.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas has special functionality for manipulating strings. Below, we use a regular expression to extract the pertinent numbers from the `aboard` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aboard.str.extract(r'(\\d+)?\\D*(\\d+)?\\D*(\\d+)?').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping into tidy form\n",
    "\n",
    "Occasionally, you will have several columns of data that all belong in a single column. Take a look at the DataFrame below on the average arrival delay of airlines at different airports. All of the columns with three-letter airport codes could be placed in the same column as they all contain the arrival delay which has the same units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aad = pd.read_csv('../data/tidy/average_arrival_delay.csv').head()\n",
    "aad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `melt` method stacks columns one on top of the other. Here, it places all of the three-letter airport code columns into a single column. The first two airports (ATL and DEN) are shown below in the new tidy DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aad.melt(id_vars='airline', var_name='airport', value_name='delay').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Data\n",
    "\n",
    "pandas can join multiple DataFrames together by matching values in one or more columns. If you are familiar with SQL, then pandas performs joins in a similar fashion. Below, we make a connection to a database and read in two of its tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///../data/databases/neurIPS.db')\n",
    "authors = pd.read_sql('Authors', engine)\n",
    "pa = pd.read_sql('PaperAuthors', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the first three rows of each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now join these tables together using the `merge` method. The `AuthorID` column from the `pa` table is aligned with the `Id` column of the `authors` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.merge(authors, how='left', left_on='AuthorId', right_on='Id').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "\n",
    "One of the original purposes of pandas was to do time series analysis. Below, we read in 20 years of Microsoft's closing stock price data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft = pd.read_csv('../data/stocks/msft20.csv', parse_dates=['date'], index_col='date')\n",
    "msft.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a period of time\n",
    "\n",
    "pandas allows us to easily select a period of time. Below, we select all of the trading data from February 27, 2017 through March 2, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft['2017-02-27':'2017-03-02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by time\n",
    "\n",
    "We can group by some length of time. Here, we group together every month of trading data and return the average closing price of that month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft_mc = msft.resample('M').agg({'close':'mean'})\n",
    "msft_mc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "pandas provides basic visualization abilities by giving its users a few default plots. Below, we plot the average monthly closing price of Microsoft for the last 20 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('../../mdap.mplstyle')\n",
    "msft_mc.plot(kind='line');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create a box plot of the average arrival delay by airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aad.plot(kind='box');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pivot table of average insurance cost by region and sex is made into a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.plot(kind='bar', rot=0).legend(bbox_to_anchor=(1, 1), loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Much More\n",
    "\n",
    "This chapter contained a small sampling from many of the major sections of the pandas library. The rest of the book focuses on going into great detail on how to effectively use the pandas library to complete nearly any kind of data analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
